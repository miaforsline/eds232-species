---
title: "Lab 1c. Species Distribution Modeling - Decision Trees"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: flatly
    code_folding: show
    toc: true
    toc_float: true
    number_sections: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE, 
                      warning = FALSE, 
                      include = TRUE,
                      quiet = TRUE)
```

# Learning Objectives 
- explore Supervised Classification for categorical values, which is a Classification 
  - Supervised Classification for continuous values is a Regression 
- use Decision Trees as a Classification technique to classify categorical species presence and absence data
  - specifically, use Recursive Partitioning and Random Forest techniques

# Load packages
- set options 
- set `ggplot` graph theme
- establish directory paths 

```{r}
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}

librarian::shelf(
  caret,       # m: modeling framework
  dplyr, ggplot2 ,here, readr, 
  pdp,         # X: partial dependence plots
  rpart,       # m: recursive partition modeling
  rpart.plot,  # m: recursive partition plotting
  rsample,     # d: split train/test data
  skimr,       # d: skim summarize data table
  vip)         # X: variable importance

options(
  scipen = 999,
  readr.show_col_types = F)
set.seed(42)

ggplot2::theme_set(ggplot2::theme_light())

dir_data    <- here("data")
pts_env_csv <- file.path(dir_data, "pts_env.csv")
```


# Decision Trees 
- read in *Necturus maculosus* data 
- clean the data
  - remove unnecessary columns 
  - change the presence/absence data to a factor 
  - drop rows with NA values 
- summarize the data using `skim()`
```{r}
pts_env <- read_csv(pts_env_csv)

d <- pts_env %>% 
  select(-ID) %>%                   # not used as a predictor x
  mutate(
    present = factor(present)) %>%  # categorical response
  na.omit()                         # drop rows with NA

skim(d)
```

## Split data into training and test data sets 
- use 80% of the data to create a training data set 
- use 20% of the data to create a testing data set 
- view how many presence and absence data points are in each set 
```{r}
d_split  <- rsample::initial_split(d, prop = 0.8, strata = "present")
d_train  <- rsample::training(d_split)

# show number of rows present is 0 vs 1
table(d$present)

table(d_train$present)
```
## Partition, depth = 1 
- run decision stump model with one split and two nodes 
- plot the decision tree 
```{r}
mod <- rpart(present ~ ., data = d_train, 
             control = list(cp = 0, minbucket = 5, maxdepth = 1))
mod

par(mar = c(1, 1, 1, 1))
rpart.plot(mod)
```

## Partition, depth = default 
- run decision tree model with default settings
- plot the decision tree 
- plot the complexity parameter
- display the cross validation results 
```{r}
mod <- rpart(present ~ ., data = d_train)
mod

rpart.plot(mod)

# plot complexity parameter
plotcp(mod)

# rpart cross validation results
mod$cptable
```

## Feature Interpretation 
- plot caret cross validation results
  - as model complexity increases, its accuracy decreases 
  - in other words, it's detrimental for a model to become too complicated
```{r}
mdl_caret <- train(
  present ~ .,
  data       = d_train,
  method     = "rpart",
  trControl  = trainControl(method = "cv", number = 10),
  tuneLength = 20)

ggplot(mdl_caret)
```
## Examine the importance of each predictor 
```{r}
vip(mdl_caret, num_features = 40, bar = FALSE)
```
# Partial dependence plots
- construct and display plots for the 3 most important predictors 
```{r}
p1 <- partial(mdl_caret, pred.var = "lat") %>% autoplot()
p2 <- partial(mdl_caret, pred.var = "WC_bio1") %>% autoplot()
p3 <- partial(mdl_caret, pred.var = c("lat", "WC_bio1")) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
              colorkey = TRUE, screen = list(z = -20, x = -60))

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

